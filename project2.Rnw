\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,float,amsfonts,enumitem,tcolorbox,hyperref}

\include{macros}

<<{r  global_options},echo=FALSE>>=
knitr::opts_chunk$set(include=TRUE,echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width=3, fig.height=3, fig.align='center',
                      fig.asp = 1
                      )
@

\title{STAT 154: Project 2 Cloud Data}
\author{Release date: \textbf{Wednesday, April 10}}
\date{Due by: \textbf{11 PM, Wednesday, May 1}}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle

\section*{Please read carefully!}
\begin{itemize}
  \item It is a good idea to revisit your notes, slides and reading;
and synthesize their main points BEFORE doing the project.
  \item \emph{For this project, we adapt a zero tolerance policy with 
  incorrect/late submissions (no emails please) to Gradescope.}
  \item The recommended work of this project is at least 20 hours (at least 10 hours / person). Plan ahead and start early. 
  \item We need two things:
  \begin{enumerate}[label=(\alph*)]
    \item A main pdf report \textbf{(font size at least 11 pt, less or equal
    to 12 pages)} generated by Latex, Rnw or Word is required to be
    submitted to Gradescope.
    \begin{itemize}
      \item Provide top class (research-paper level) writing, useful
    well-labeled figures and no code in this pdf. Arrange text and figures
    compactly (.Rnw may not be very useful for this).
    \item You can choose a title for the report and a team name as per your
    liking (\emph{get creative!}). Do provide the names and student ID of
    your teammates below the title.
    \item Your report should conclude with an acknowledgment section, where
    you provide brief discussion about the contributions of each member,
    \textbf{and} the resources you used, credit all the help you took
    and briefly outline the way you proceeded with the project.
    \end{itemize}
    \item A link to your GitHub Repo at the end of your write-up that contains
    all your code (see Section 5 for more details).
  \end{enumerate}
  \item \textbf{Be visual \emph{and} quantitative:} Remember projects are graded differently when compared to homework---one line answer without explanation is usually not enough. Make your findings succinct and try to convince us with good arguments supported by numbers and figures.
Putting yourself in reader's shoes and reading the report out loud usually helps. The standards for grading are \emph{very high} this time. We will be very picky with figures: Lack of proper titles and axis labels will lead to loss of several points.
  
\end{itemize}

\newpage

\section*{Overview of the project} % (fold)
\label{sec:overview_of_the_project}

The goal of this project is the exploration and modeling of cloud detection in 
the polar regions based on radiance recorded automatically by the MISR sensor 
abroad the NASA satellite Terra. You will attempt to build a classification 
model to distinguish the presence of cloud from the absence of clouds in
the images using the available signals/features. Your dataset has ``expert
labels'' that can be used to train your models. When you evaluate your
results, imagine that your models will be used to distinguish
clouds from non-clouds on a large number of images that won't have these 
``expert'' labels.

On Piazza, you will find a zip archive with three files: \textbf{image1.txt},
\textbf{image2.txt}, \textbf{image3.txt}. Each contains one picture from
the satellite. Each of these files contains several rows each with 11 columns 
described in the Table~\ref{tab:feature} below. All five radiance angles
are raw features, while NDAI, SD, and CORR are features that are computed
based on subject matter knowledge. More information about the features is
in the article \textbf{yu2008.pdf}. The sensor data is multi-angle and recorded
in the red-band. 
For more information about MISR, see \textbf{http://www-misr.jpl.nasa.gov/}.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
 01 & y coordinate \\ \hline
 02 & x coordinate \\ \hline
 03 & expert label (+1 = cloud, -1 = not cloud, 0 unlabeled)\\ \hline
 04 & NDAI \\ \hline
 05 & SD \\ \hline
 06 & CORR \\ \hline
 07 & Radiance angle DF\\ \hline
 08 & Radiance angle CF\\ \hline
 09 & Radiance angle BF\\ \hline
 10 & Radiance angle AF\\ \hline
 11 & Radiance angle AN\\ \hline
\end{tabular}
\label{tab:feature}
\caption{Features in the cloud data.}
\end{table}
<<>>=Importing libraries
library(ggplot2)
library(gridExtra)
library(GGally)
library(dplyr)
library(caret)
library(overlapping)
library(lattice)
library(pROC)
library(rpart.plot)
library(tidyverse)
library(car)
library(MASS)
library(bmrm)
library(rocc)
library(Metrics)
library(LiblineaR)
library(rpart)
setwd("C:/Users/Tiffy/Desktop/Stat 154/Project 2/image_data")
@


<<>>=Read in the tables
img1_df = read.table("image1.txt", header = FALSE)
img2_df = read.table("image2.txt", header = FALSE)
img3_df = read.table("image3.txt", header = FALSE)

rename_cols = function(df) {
  colnames = c("y", "x", "expertlabel", "NDAI", "SD", "CORR",
               "DF", "CF","BF", "AF", "AN")
  for (i in 1:11) {
    names(df)[i] = colnames[i]
  }
  return (df)
}

img1_df = rename_cols(img1_df)
img2_df = rename_cols(img2_df)
img3_df = rename_cols(img3_df)
@



\section{Data Collection and Exploration (30 pts)}
\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a half-page summary} of the paper, including at least
the purpose of the study, the data, the collection method, its conclusions
and potential impact.


\item \textbf{Summarize} the data, i.e., $\%$ of pixels for the different
classes. \textbf{Plot well-labeled beautiful maps} using $x, y$ coordinates
the expert labels with color of the region based on the expert labels.
\textbf{Do you observe some trend/pattern? Is an i.i.d. assumption for
the samples justified for this dataset?}
<<>>= Summarize the data
summary(img1_df)
summary(img2_df)
summary(img3_df)
#% of pixels that are cloudy
pixel_percentage = function(df) {
  cloudy = df[df$expertlabel == 1,]
  unlabeled = df[df$expertlabel == 0,]
  uncloudy = df[df$expertlabel == -1,]
  
  cloudy_percent = nrow(cloudy)/nrow(df)
  unlabeled_percent = nrow(unlabeled)/nrow(df)
  uncloudy_percent = nrow(uncloudy)/nrow(df)
  return (c(cloudy_percent, unlabeled_percent, 
            uncloudy_percent))
}
img1_summary = pixel_percentage(img1_df)
img2_summary = pixel_percentage(img2_df)
img3_summary = pixel_percentage(img3_df)
print(img1_summary)
print(img2_summary)
print(img3_summary)
@

<<>>= Plotting
img1_plot = ggplot(img1_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme_grey(base_size = 14)

img2_plot = ggplot(img2_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)

img3_plot = ggplot(img3_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)
grid.arrange(img1_plot, img2_plot, img3_plot, ncol = 2, nrow = 2)
@

\begin{tcolorbox}
We see that if a cloud exists on a pixel then the nearby pixels are also very likely to have a cloud. In other words, cloudy regions are typically together and so are non-cloudy regions. An iid assumption is not justified for this data.
\end{tcolorbox}


\item \textbf{Perform a visual and quantitative EDA} of the dataset, e.g.,
summarizing (i) pairwise relationship between the features themselves and
(ii) the relationship between the expert labels with the individual features.
\textbf{Do you notice differences} between the two classes (cloud, no cloud)
based on the radiance or other features (CORR, NDAI, SD)?
\end{enumerate}
<<>>=Pairs for image 1
pairs(img1_df)
@

<<>>=Pairs for image 2
pairs(img2_df)
@

<<>>=Pairs for image 3
pairs(img3_df)
@

<<>>=Plotting functions (pairwise scatter)
plot_NDAI_SD = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = SD, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "SD", color = "Expert Label")
  return(plot)
}

plot_NDAI_CORR = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = CORR, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "CORR",
       color = "Expert Label")
  return(plot)
}

plot_NDAI_DF = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = DF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "Radiance angle DF",
       color = "Expert Label")
  return(plot)
}

plot_NDAI_CF = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = CF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "Radiance angle CF",
       color = "Expert Label")
  return(plot)
}

plot_NDAI_BF = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = BF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "Radiance angle BF",
       color = "Expert Label")
  return(plot)
}

plot_NDAI_AF = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = AF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "Radiance angle AF",
       color = "Expert Label")
  return(plot)
}

plot_NDAI_AN = function(df) {
  plot = ggplot(df, aes(x = NDAI, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "NDAI", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}

plot_SD_CORR = function(df) {
  plot = ggplot(df, aes(x = SD, y = CORR, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "SD", y = "CORR",
       color = "Expert Label")
  return(plot)
}

plot_SD_DF = function(df) {
  plot = ggplot(df, aes(x = SD, y = DF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "SD", y = "Radiance angle DF",
       color = "Expert Label")
  return(plot)
}

plot_SD_CF = function(df) {
  plot = ggplot(df, aes(x = SD, y = CF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "SD", y = "Radiance angle CF",
       color = "Expert Label")
  return(plot)
}

plot_SD_BF = function(df) {
  plot = ggplot(df, aes(x = SD, y = BF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "SD", y = "Radiance angle BF",
       color = "Expert Label")
  return(plot)
}

plot_SD_AF = function(df) {
  plot = ggplot(df, aes(x = SD, y = AF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "SD", y = "Radiance angle AF",
       color = "Expert Label")
  return(plot)
}

plot_SD_AN = function(df) {
  plot = ggplot(df, aes(x = SD, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "SD", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}

plot_CORR_DF = function(df) {
  plot = ggplot(df, aes(x = CORR, y = DF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "CORR", y = "Radiance angle DF",
       color = "Expert Label")
  return(plot)
}

plot_CORR_CF = function(df) {
  plot = ggplot(df, aes(x = CORR, y = CF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "CORR", y = "Radiance angle CF",
       color = "Expert Label")
  return(plot)
}

plot_CORR_BF = function(df) {
  plot = ggplot(df, aes(x = CORR, y = BF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "CORR", y = "Radiance angle BF",
       color = "Expert Label")
  return(plot)
}

plot_CORR_AF = function(df) {
  plot = ggplot(df, aes(x = CORR, y = AF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "CORR", y = "Radiance angle AF",
       color = "Expert Label")
  return(plot)
}

plot_CORR_AN = function(df) {
  plot = ggplot(df, aes(x = CORR, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "CORR", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}

plot_DF_CF = function(df) {
  plot = ggplot(df, aes(x = DF, y = CF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle DF", y = "Radiance angle CF",
       color = "Expert Label")
  return(plot)
}

plot_DF_BF = function(df) {
  plot = ggplot(df, aes(x = DF, y = BF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle DF", y = "Radiance angle BF",
       color = "Expert Label")
  return(plot)
}

plot_DF_AF = function(df) {
  plot = ggplot(df, aes(x = DF, y = AF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle DF", y = "Radiance angle AF",
       color = "Expert Label")
  return(plot)
}

plot_DF_AN = function(df) {
  plot = ggplot(df, aes(x = DF, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle DF", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}

plot_CF_BF = function(df) {
  plot = ggplot(df, aes(x = CF, y = BF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle CF", y = "Radiance angle BF",
       color = "Expert Label")
  return(plot)
}

plot_CF_AF = function(df) {
  plot = ggplot(df, aes(x = cF, y = AF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle CF", y = "Radiance angle AF",
       color = "Expert Label")
  return(plot)
}

plot_CF_AN = function(df) {
  plot = ggplot(df, aes(x = cF, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle CF", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}

plot_BF_AF = function(df) {
  plot = ggplot(df, aes(x = BF, y = AF, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle BF", y = "Radiance angle AF",
       color = "Expert Label")
  return(plot)
}

plot_BF_AN = function(df) {
  plot = ggplot(df, aes(x = BF, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle BF", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}

plot_AF_AN = function(df) {
  plot = ggplot(df, aes(x = AF, y = AN, color = factor(expertlabel))) +
    geom_point() + 
    scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Radiance angle AF", y = "Radiance angle AN",
       color = "Expert Label")
  return(plot)
}
@

<<>>=Plot the ones with relationships that we can see in pairs to investigate some more 
img1_CORR_AN = plot_CORR_AN(img1_df) + labs(title = "Image 1 CORR vs AN")
img2_CORR_AN = plot_CORR_AN(img2_df) + labs(title = "Image 2 CORR vs AN")
img3_CORR_AN = plot_CORR_AN(img3_df) + labs(title = "Image 3 CORR vs AN")
grid.arrange(img1_CORR_AN, img2_CORR_AN, img3_CORR_AN, ncol = 2, nrow = 2)
@

<<>>=DF vs CF
img1_DF_CF = plot_DF_CF(img1_df) + labs(title = "Image 1 DF vs CF")
img2_DF_CF = plot_DF_CF(img2_df) + labs(title = "Image 2 DF vs CF")
img3_DF_CF = plot_DF_CF(img3_df) + labs(title = "Image 3 DF vs CF")
grid.arrange(img1_DF_CF, img2_DF_CF, img3_DF_CF, ncol = 2, nrow = 2)
@

<<>>=AF vs AN
img1_AF_AN = plot_AF_AN(img1_df) + labs(title = "Image 1 AF vs AN")
img2_AF_AN = plot_AF_AN(img2_df) + labs(title = "Image 1 AF vs AN")
img3_AF_AN = plot_AF_AN(img3_df) + labs(title = "Image 1 AF vs AN")
grid.arrange(img1_AF_AN, img2_AF_AN, img3_AF_AN, ncol = 2, nrow = 2)
@


<<>>= Aggregate all the data into one
img_df = rbind(img1_df, img2_df, img3_df)
@

<<>>= Correlation for all 3 images
cor(img_df)
@

<<>>= Plot relative
xcoord = ggplot(img_df, aes(x = expertlabel, y = x, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "x-coordinate") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

ycoord = ggplot(img_df, aes(x = expertlabel, y = y, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "y-coordinate") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

NDAI = ggplot(img_df, aes(x = expertlabel, y = NDAI, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "NDAI") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

SD = ggplot(img_df, aes(x = expertlabel, y = SD, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "SD") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

CORR = ggplot(img_df, aes(x = expertlabel, y = CORR, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "CORR") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

DF = ggplot(img_df, aes(x = expertlabel, y = DF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle DF") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

CF = ggplot(img_df, aes(x = expertlabel, y = CF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle CF") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

BF = ggplot(img_df, aes(x = expertlabel, y = BF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle BF") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

AF = ggplot(img_df, aes(x = expertlabel, y = AF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle AF") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

AN = ggplot(img_df, aes(x = expertlabel, y = AN, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle AN") + 
  theme(axis.ticks.x=element_blank(), axis.text.x=element_blank(), legend.title = element_blank())

grid.arrange(xcoord, ycoord, NDAI, CORR, SD,
             DF, CF, BF, AF, AN, ncol = 3, nrow = 4)
@

\section{Preparation (40 pts)}
Now that we have done EDA with the data, we now prepare to train our model.
\begin{enumerate}[label=(\alph*)]
\item (Data Split) \textbf{Split the entire data} (image1.txt, image2.txt,
image3.txt) into three sets: training,  validation and test. Think carefully
about how to split the data. \textbf{Suggest at least two non-trivial different
ways} of splitting the data which takes into account that the data is not i.i.d.

<<>>=Visually inspect splits
img1_grid = ggplot(img1_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme(
    panel.grid.major = element_line(color = 'black', size = 2, linetype = 'solid'),
    panel.grid.minor = element_line(color = 'black', size = 2, linetype = 'solid')
  )
  

img2_grid = ggplot(img2_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme(
    panel.grid.major = element_line(color = 'black', size = 2, linetype = 'solid'),
    panel.grid.minor = element_line(color = 'black', size = 2, linetype = 'solid')
  )

img3_grid = ggplot(img3_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme(
    panel.grid.major = element_line(color = 'black', size = 2, linetype = 'solid'),
    panel.grid.minor = element_line(color = 'black', size = 2, linetype = 'solid')
  )

grid.arrange(img1_grid, img2_grid, img3_grid, ncol = 2, nrow = 2)
@

<<>>=50x50 split
grid_split = function(df) {
  divisions = matrix(seq(1,56,1), nrow = 8, ncol = 7, byrow = TRUE)
  regions = c()
  for (i in 1:nrow(df)) {
    row = df[i,]
    regions = c(regions, divisions[8 - (floor(row$y/50)), floor(row$x/50)])
  }
  return(regions)
}

img1_df$regions = grid_split(img1_df)
img2_df$regions = grid_split(img2_df)
img3_df$regions = grid_split(img3_df)

set.seed(1)

grid_train = function(df) {
  divisions = seq(1,56,1)
  train_idx = c()
  for (i in divisions) {
    subset = which(df$regions == i)
    train_idx = c(train_idx, sample(subset, size = floor(0.80 * length(subset))))
  }
  train = df[train_idx,]
  test = df[-train_idx,]
  return(list("train" = train, "test" = test))
}

img1_labels = filter(img1_df, expertlabel == -1 | expertlabel == 1 )
img2_labels = filter(img2_df, expertlabel == -1 | expertlabel == 1 )
img3_labels = filter(img3_df, expertlabel == -1 | expertlabel == 1 )
@

<<>>=Split into train, validation, test, method 1
split1 = grid_train(img1_labels)
split2 = grid_train(img2_labels)
split3 = grid_train(img3_labels)

test = rbind(split1$test, split2$test, split3$test)
train_val = rbind(split1$train, split2$train, split3$train)
rownames(test) = NULL
rownames(train_val) = NULL

split_val = grid_train(train_val)
train = split_val$train
val = split_val$test
@

<<>>=X-axis split
x_train = function(df) {
  x_range = range(df$x)
  x_range = seq(x_range[1],x_range[2],1)
  train_idx = c()
  for (i in x_range) {
    subset = which(df$x == i)
    train_idx = c(train_idx, sample(subset, size = floor(0.80 * length(subset))))
  }
  train = df[train_idx,]
  test = df[-train_idx,]
  return(list("train" = train, "test" = test))
}
@

<<>>=Split into train, validation, test, method 2
split4 = x_train(img1_labels)
split5 = x_train(img2_labels)
split6 = x_train(img3_labels)

test2 = rbind(split4$test, split5$test, split6$test)
train_val2 = rbind(split4$train, split5$train, split6$train)
rownames(test2) = NULL
rownames(train_val2) = NULL

split_val2 = grid_train(train_val2)
train2 = split_val2$train
val2 = split_val2$test
@


<<>>=Combine all data
#For the sake of ease let's just aggregate the data
img_df = rbind(img1_df, img2_df, img3_df)
@

\item (Baseline) \textbf{Report the accuracy of a trivial classifier} which
sets all labels to -1 (cloud-free) on the validation set and on the test set. 
In what scenarios will such a classifier have high average accuracy?
\emph{Hint: Such a step provides a baseline to ensure that the classification
problems at hand is not trivial.}

<<>>= Classify all to -1(noncloudy on test and validation)
#Method 1
val_1_acc = nrow(val[val$expertlabel == -1,])/nrow(val)
test_1_acc = nrow(test[test$expertlabel == -1,])/nrow(test)

val_2_acc = nrow(val2[val2$expertlabel == -1,])/nrow(val2)
test_2_acc = nrow(test2[test2$expertlabel == -1,])/nrow(test2)

print(val_1_acc)
print(test_1_acc)
print(val_2_acc)
print(test_2_acc)
@

\item (First order importance) Assuming the expert labels as the
truth, and without using fancy classification methods, suggest
three of the ``best'' features, \textbf{using quantitative and visual justification}. Define your ``best'' feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.

<<>>= Get some quantitative information
cloudy_img_df = img_df[img_df$expertlabel == 1, ]
uncloudy_img_df = img_df[img_df$expertlabel == -1, ]

mean_sd = function(x) {
  list(mean(x), sd(x), mean(x) - sd(x), mean(x) + sd(x))
}

sapply(cloudy_img_df, mean_sd)
sapply(uncloudy_img_df, mean_sd)

#Calculating the overlap using density estimation from other packages
overlap_type = c()
overlap_percent = c()

find_overlap = function(type, df1_feature, df2_feature) {
  overlap_type = list(df1_feature, df2_feature)
  overlap_type = overlap(overlap_type)
  return (list(Type = type, Percent = overlap_type$OV[1]))
}

NDAI_overlap = find_overlap("NDAI", cloudy_img_df$NDAI,
                            uncloudy_img_df$NDAI)
x_overlap = find_overlap("x", cloudy_img_df$x, 
                         uncloudy_img_df$x)
CORR_overlap = find_overlap("CORR", cloudy_img_df$CORR, 
                            uncloudy_img_df$CORR)
AF_overlap = find_overlap("AF", cloudy_img_df$AF, 
                          uncloudy_img_df$AF)
AN_overlap = find_overlap("AN", cloudy_img_df$AN, 
                          uncloudy_img_df$AN)
SD_overlap = find_overlap("SD", cloudy_img_df$SD, 
                          uncloudy_img_df$SD)
overlap_df = rbind(NDAI_overlap, x_overlap, CORR_overlap,
                   AF_overlap, AN_overlap, SD_overlap)
overlap_df
@

<<>>=Visually check out the density curves of our candidates
label_img_df = filter(img_df, expertlabel == 1 | expertlabel == -1)
NDAI_density = ggplot(label_img_df, aes(x = NDAI, fill = factor(expertlabel))) +
  geom_density(alpha = 0.4) +
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "NDAI")

x_density = ggplot(label_img_df, aes(x = x, fill = factor(expertlabel))) +
  geom_density(alpha = 0.4) +
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "x-coordinate")

CORR_density = ggplot(label_img_df, aes(x = CORR, fill = factor(expertlabel))) +
  geom_density(alpha = 0.4) +
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "CORR")

SD_density = ggplot(label_img_df, aes(x = SD, fill = factor(expertlabel))) +
  geom_density(alpha = 0.4) +
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  xlim(c(0,30)) + 
  labs(x = "SD")

AF_density = ggplot(label_img_df, aes(x = AF, fill = factor(expertlabel))) +
  geom_density(alpha = 0.4) +
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Radiance Angle AF")

AN_density = ggplot(label_img_df, aes(x = AN, fill = factor(expertlabel))) +
  geom_density(alpha = 0.4) +
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Radiance Angle AN")

grid.arrange(NDAI_density, x_density, CORR_density,
             AF_density, AN_density, SD_density, nrow = 2, ncol = 3)
@

<<>>=Check Correlation between the three
cor(img_df$NDAI, img_df$SD)
cor(img_df$NDAI, img_df$CORR)
cor(img_df$SD, img_df$CORR)
@

\item Write a generic cross validation (CV) function \textbf{CVgeneric} in R that takes a generic classifier, training features, training labels, number of folds $K$ and a loss function (at least classification accuracy should be there) as inputs and outputs the $K$-fold CV loss on the training set.  Please remember to put it in your github folder in Section 5.
\end{enumerate}

<<>>=Fold creation 
#Create folds based off the passed in training set 
#Folds are saved into the original dataframe with their fold number
CVfold = function(train, K, block) {
  lengths = c()
  folds = c()
  #Block method 
  if (block) {
    divisions = seq(1,56,1)
    #Saving the size of each division
    for(division in divisions) {
      lengths[division] <- length(which(train$regions==division))
    }
    set.seed(1)
    #Repeatedly sample 1/K from each region
    #Remove that row once it has been selected
    for (division in divisions) {
      subset = which(train$regions==division)
      for (fold in 1:K) {
        sample_idx = sample(subset, 
                             size = floor(lengths[division] / K))
        for (i in sample_idx){
          folds[i] = fold
        }
        subset = subset[!subset %in% sample_idx]
      }
    }
  #X-axis method
  } else {
    divisions = range(train$x)
    divisions = seq(divisions[1], divisions[2], 1)
    for(division in divisions) {
      lengths[division] <- length(which(train$x==division))
    }
    set.seed(1)
    #Repeatedly sample 1/K from each region
    #Remove that row once it has been selected
    for (division in divisions) {
      subset = which(train$x==division)
      for (fold in 1:K) {
        sample_idx = sample(subset, 
                             size = floor(lengths[division] / K))
        for (i in sample_idx){
          folds[i] = fold
        }
        subset = subset[!subset %in% sample_idx]
      }
    }
  }
  folds[is.na(folds)] = K
  return(folds)
}
@

<<>>=Generic CV Function
genericCV <- function(classifier, trainingfeatures, traininglabel, K = 10, 
                      loss = "mse", data, grid = NULL) {
   CVmodels = list()
   CVloss = list()
   
   # create a loss function
   lossfun <- function(type = loss, mod) {
     if (type == "mse"){
       a = mean( predict(mod, newdata = data) != data[,traininglabel] )
       return(a)
     }
     else { #depends on which loss function to use
       return(type(mod$pred$pred, mod$pred$obs))
     }
   }
   
   for (i in 1:K) { # create a model trained on 9 different folds
    fxn <- as.formula(paste(traininglabel,
                            "~", 
                            paste(trainingfeatures, collapse = " + ")))
    print(fxn)
    mod = train(fxn, 
                data = data[which(data$folds == i),],
                method = classifier,
                preProcess = c("center", "scale"), tuneGrid = grid
                )
   CVmodels[[paste0("fold",i,".model", collapse = "")]] = mod
   CVloss[[paste0("fold",i,".loss",collapse = "")]] = lossfun(mod=CVmodels[[i]])
   }
   
   return(list("models" = CVmodels, "loss" = CVloss))
}
@

<<>>=Blocking method testing
train_val$expertlabel = as.factor(train_val$expertlabel)
traininglabel <- "expertlabel"
features <- c("y", "x", "NDAI","SD","CORR", "DF", "CF", "BF", "AF", "AN")
set.seed(1)
train_val$folds = CVfold(train_val, 10, block = TRUE)
@


\section{Modeling (40 pts)}
We now try to fit different classification models and assess the fitted
models using different criterion. For the next three parts, we expect you
to try \emph{logistic regression and at least three other methods}.
\begin{enumerate}[label=(\alph*)]
\item \textbf{Try several classification methods and assess their fit using
cross-validation (CV). Provide a commentary on the assumptions for the
methods you tried and if they are satisfied in this case.}
Since CV does not have a validation set, you can merge your training and
validation set to fit your CV model. 

<<>>=Logistic Regression Block
set.seed(1)
block_log = genericCV('glm', features, traininglabel,
          K = 10, data = train_val)
@

<<>>=Best Logistic Regression Block
block_accuracy = c()
for (model in block_log$models){
  block_accuracy= c(block_accuracy, model$results[2])
}
best_block_model = block_log$models[which.max(block_accuracy)]

block_predictions = unlist(predict(best_block_model, newdata = test), use.names = FALSE)
test$block_predict = block_predictions
block_acc =  sum(test$block_predict == test$expertlabel) / nrow(test)
print(block_acc)
print(nrow(test[test$block_predict == 1 & test$expertlabel == 1,]) / nrow(test[test$expertlabel == 1,]))
print(nrow(test[test$block_predict == -1 & test$expertlabel == -1,]) / nrow(test[test$expertlabel == -1,]))
@

<<>>= Feature importance
ft_imp = train(expertlabel ~ y + x + NDAI + SD + CORR + DF + CF + BF + AF + AN,
               data = train_val[which(train_val$folds == 9),],
               method = "glm",
               preProcess = c("center", "scale")
    )
exp(coef(ft_imp$finalModel))
@

<<>>=Multicollinearity
vif(best_block_model$fold9.model$finalModel)
@


<<>>=Plot prediction logistic models next to correct
block_predictions = unlist(predict(best_block_model, newdata = img1_df), use.names = FALSE)
img1_df$block_log_predict = block_predictions
block_predictions = unlist(predict(best_block_model, newdata = img2_df), use.names = FALSE)
img2_df$block_log_predict = block_predictions
block_predictions = unlist(predict(best_block_model, newdata = img3_df), use.names = FALSE)
img3_df$block_log_predict = block_predictions

img1_pred = ggplot(img1_df, aes(x = x, y = y, color = factor(block_log_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img2_pred = ggplot(img2_df, aes(x = x, y = y, color = factor(block_log_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img3_pred = ggplot(img3_df, aes(x = x, y = y, color = factor(block_log_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img1_plot = ggplot(img1_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme_grey(base_size = 14)

img2_plot = ggplot(img2_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)

img3_plot = ggplot(img3_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)
grid.arrange(img1_plot, img1_pred, img2_plot, img2_pred, img3_plot, img3_pred, ncol = 2, nrow = 3)
@

<<>>=Random Decision Tree Block
set.seed(3)
block_tree = genericCV('rpart', features,
                         traininglabel,
                         K = 10, data = train_val)
@

<<>>=Best Decision Tree Block 
block_accuracy = c()
for (model in block_tree$models){
  block_accuracy= c(block_accuracy, max(model$results[2]))
}
best_block_model = block_tree$models[which.max(block_accuracy)]

block_predictions = unlist(predict(best_block_model, newdata = test), use.names = FALSE)
test$block_predict = block_predictions
block_acc =  (sum(test$block_predict == test$expertlabel) / nrow(test))
print(block_acc)
print(nrow(test[test$block_predict == 1 & test$expertlabel == 1,]) / nrow(test[test$expertlabel == 1,]))
print(nrow(test[test$block_predict == -1 & test$expertlabel == -1,]) / nrow(test[test$expertlabel == -1,]))
@

<<>>=Decision Tree Graphic
rpart.plot(best_block_model$fold3.model$finalModel, box.palette="RdBu", shadow.col="gray", nn=TRUE)
@

<<>>=Plot prediction decision tree models next to correct
block_predictions = unlist(predict(best_block_model, newdata = img1_df), use.names = FALSE)
img1_df$block_tree_predict = block_predictions
block_predictions = unlist(predict(best_block_model, newdata = img2_df), use.names = FALSE)
img2_df$block_tree_predict = block_predictions
block_predictions = unlist(predict(best_block_model, newdata = img3_df), use.names = FALSE)
img3_df$block_tree_predict = block_predictions

img1_pred = ggplot(img1_df, aes(x = x, y = y, color = factor(block_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img2_pred = ggplot(img2_df, aes(x = x, y = y, color = factor(block_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img3_pred = ggplot(img3_df, aes(x = x, y = y, color = factor(block_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img1_plot = ggplot(img1_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme_grey(base_size = 14)

img2_plot = ggplot(img2_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)

img3_plot = ggplot(img3_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)
grid.arrange(img1_plot, img1_pred, img2_plot, img2_pred, img3_plot, img3_pred, ncol = 2, nrow = 3)
@

<<>>=X axis method testing
train_val2$expertlabel = as.factor(train_val2$expertlabel)
set.seed(1)
train_val2$folds = CVfold(train_val2, 10, block = FALSE)
@

<<>>=Logistic Regression X
set.seed(1)
x_log = genericCV('glm', features, traininglabel,
          K = 10, data = train_val)
@

<<>>=Best Logistic Regression X
x_accuracy = c()
for (model in x_log$models){
  x_accuracy= c(x_accuracy, model$results[2])
}
best_x_model = x_log$models[which.max(x_accuracy)]

x_predictions = unlist(predict(best_x_model, newdata = test), use.names = FALSE)
test$x_predict = x_predictions
x_acc =  (sum(test$x_predict == test$expertlabel) / nrow(test))
print(x_acc)
print(nrow(test[test$x_predict == 1 & test$expertlabel == 1,]) / nrow(test[test$expertlabel == 1,]))
print(nrow(test[test$x_predict == -1 & test$expertlabel == -1,]) / nrow(test[test$expertlabel == -1,]))
@

<<>>= Feature importance
ft_imp = train(expertlabel ~ y + x + NDAI + SD + CORR + DF + CF + BF + AF + AN,
               data = train_val[which(train_val2$folds == 9),],
               method = "glm",
               preProcess = c("center", "scale")
    )
exp(coef(ft_imp$finalModel))
@

<<>>=Plot prediction logistic models next to correct
x_predictions = unlist(predict(best_x_model, newdata = img1_df), use.names = FALSE)
img1_df$x_log_predict = x_predictions
x_predictions = unlist(predict(best_x_model, newdata = img2_df), use.names = FALSE)
img2_df$x_log_predict = x_predictions
x_predictions = unlist(predict(best_x_model, newdata = img3_df), use.names = FALSE)
img3_df$x_log_predict = x_predictions

img1_pred = ggplot(img1_df, aes(x = x, y = y, color = factor(x_log_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img2_pred = ggplot(img2_df, aes(x = x, y = y, color = factor(x_log_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img3_pred = ggplot(img3_df, aes(x = x, y = y, color = factor(x_log_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img1_plot = ggplot(img1_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme_grey(base_size = 14)

img2_plot = ggplot(img2_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)

img3_plot = ggplot(img3_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)
grid.arrange(img1_plot, img1_pred, img2_plot, img2_pred, img3_plot, img3_pred, ncol = 2, nrow = 3)
@

<<>>=Random Decision Tree X
set.seed(1)
x_tree = genericCV('rpart', features,
                         traininglabel,
                         K = 10, data = train_val2)
@

<<>>=Best Decision Tree X 
x_accuracy = c()
for (model in x_tree$models){
  x_accuracy= c(x_accuracy, max(model$results[2]))
}
best_x_model = x_tree$models[which.max(x_accuracy)]

x_predictions = unlist(predict(best_x_model, newdata = test), use.names = FALSE)
test$x_predict = x_predictions
x_acc =  (sum(test$x_predict == test$expertlabel) / nrow(test))
print(x_acc)
print(nrow(test[test$x_predict == 1 & test$expertlabel == 1,]) / nrow(test[test$expertlabel == 1,]))
print(nrow(test[test$x_predict == -1 & test$expertlabel == -1,]) / nrow(test[test$expertlabel == -1,]))
@

<<>>=Decision Tree Graphic
rpart.plot(best_x_model$fold7.model$finalModel, box.palette="RdBu", shadow.col="gray", nn=TRUE)
@

<<>>=Plot prediction decision tree models next to correct
x_predictions = unlist(predict(best_x_model, newdata = img1_df), use.names = FALSE)
img1_df$x_tree_predict = x_predictions
x_predictions = unlist(predict(best_x_model, newdata = img2_df), use.names = FALSE)
img2_df$x_tree_predict = x_predictions
x_predictions = unlist(predict(best_x_model, newdata = img3_df), use.names = FALSE)
img3_df$x_tree_predict = x_predictions

img1_pred = ggplot(img1_df, aes(x = x, y = y, color = factor(x_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img2_pred = ggplot(img2_df, aes(x = x, y = y, color = factor(x_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img3_pred = ggplot(img3_df, aes(x = x, y = y, color = factor(x_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predictions") + 
  theme_grey(base_size = 14)

img1_plot = ggplot(img1_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label") + 
  theme_grey(base_size = 14)

img2_plot = ggplot(img2_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)

img3_plot = ggplot(img3_df, aes(x = x, y = y, color = factor(expertlabel))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#E7B800", "#FC4E07"),
                      labels = c("Not cloudy", "No label", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Expert Label")+ 
  theme_grey(base_size = 14)
grid.arrange(img1_plot, img1_pred, img2_plot, img2_pred, img3_plot, img3_pred, ncol = 2, nrow = 3)
@

<<>>=LDA with all features used with blocking method
set.seed(1)
train_val$label = as.factor(train_val$expertlabel)
traininglabel <- "label"
train_val$folds = CVfold(train_val, 10, block = TRUE)

features <- c("y", "x", "NDAI","SD","CORR", "DF", "CF", "BF", "AF", "AN")

lda.all.block <- genericCV('lda', features, traininglabel,
          K = 10, data = train_val)
qda.all.block <- genericCV('qda', features, traininglabel,
          K = 10, data = train_val)
@

<<>>=QDA with all features used with x-axis method
set.seed(2)
train_val2$label = as.factor(train_val2$expertlabel)
traininglabel <- "label"
train_val2$folds = CVfold(train_val2, 10, block = TRUE)

features <- c("y", "x", "NDAI","SD","CORR", "DF", "CF", "BF", "AF", "AN")

lda.all.xaxis <- genericCV('lda', features, traininglabel,
          K = 10, data = train_val2)
qda.all.xaxis <- genericCV('qda', features, traininglabel,
          K = 10, data = train_val2)
@

<<>>==report lda and qda accuracies across fold and test accuracies
for (i in 1:10){
  print(lda.all.block$models[[i]]$results$Accuracy)
}

for (i in 1:10){
  print(qda.all.block$models[[i]]$results$Accuracy)
}

mean( predict(lda.all.block$models[[8]], newdata = test) == test$expertlabel)

mean( predict(qda.all.block$models[[9]], newdata = test) == test$expertlabel)
@

<<>>==investiagate feature importance and how different it gets using a different combination of features
features <- c("y", "x", "NDAI","SD","CORR")
lda.selected.block <- genericCV('lda', features, traininglabel,
          K = 10, data = train_val)
qda.selected.block <- genericCV('qda', features, traininglabel,
          K = 10, data = train_val)

features <- c("y", "x")
lda.xy.block <- genericCV('lda', features, traininglabel,
          K = 10, data = train_val)
qda.xy.block <- genericCV('lda', features, traininglabel,
          K = 10, data = train_val)

features <- c("NDAI","SD","CORR")
lda.nsc.block<- genericCV('lda', features, traininglabel,
          K = 10, data = train_val)
qda.nsc.block <- genericCV('qda', features, traininglabel,
          K = 10, data = train_val)

top3features <- c("NDAI","SD","x")
lda.top3.block<- genericCV('lda', top3features, traininglabel,
          K = 10, data = train_val)
qda.top3.block <- genericCV('qda', top3features, traininglabel,
          K = 10, data = train_val)

@

<<>>== compare accuracies with different features used
all <- c()
selected <- c()
xy <- c()
nsc <- c()
top3 <- c()
means <- list()

for (i in 1:10){
  all <- c(all, lda.all.block$models[[i]]$results$Accuracy)
  selected <- c(selected, lda.selected.block$models[[i]]$results$Accuracy)
  xy <- c(xy, lda.xy.block$models[[i]]$results$Accuracy)
  nsc <- c(nsc, lda.nsc.block$models[[i]]$results$Accuracy)
  top3 <- c(top3, lda.top3.block$models[[i]]$results$Accuracy)
  means <- list("all" = mean(all), "selected" = mean(selected), "xy" = mean(xy), "nsc" = mean(nsc), "top3" = mean(top3))
}
print(means)

all <- c()
selected <- c()
xy <- c()
nsc <- c()
top3 <- c()
means <- list()

for (i in 1:10){
  all <- c(all, qda.all.block$models[[i]]$results$Accuracy)
  selected <- c(selected, qda.selected.block$models[[i]]$results$Accuracy)
  xy <- c(xy, qda.xy.block$models[[i]]$results$Accuracy)
  nsc <- c(nsc, qda.nsc.block$models[[i]]$results$Accuracy)
  top3 <- c(top3, qda.top3.block$models[[i]]$results$Accuracy)

  means <- list("all" = mean(all), "selected" = mean(selected), "xy" = mean(xy), "nsc" = mean(nsc), "top3" = mean(top3))
}
print(means)

@


\textbf{Report} the accuracies across
folds (and not just the average across folds) and the test accuracy. CV-results
for both the ways of creating folds (as answered in part 2(a)) should be
reported. Provide a brief commentary on the results. Make sure you honestly
mention all the classification methods you have  tried.
\item \textbf{Use ROC curves to compare the different methods.} 
Choose a cutoff value and highlight it on the ROC curve. Explain your choice
of the cutoff value.

<<>>=ROC curves
par(mfrow=c(2,2))
log_block_plot = plot.roc(train_val[,traininglabel],
                       as.numeric(predict(block_log$models[[9]],
                                          newdata = train_val, 
                                          probability = TRUE)),
         print.auc = TRUE,  
         main = "Logistic Regression with Blocked Data")
points(log_block_plot$specificities[2], log_block_plot$sensitivities[2],
       col = "red", pch=16)


tree_block_plot = plot.roc(train_val[,traininglabel],
                       as.numeric(predict(block_tree$models[[3]], 
                                          newdata = train_val, 
                                          probability = TRUE)),
         print.auc = TRUE,  
         main = "Decision Tree with Blocked Data")
points(tree_block_plot$specificities[2], tree_block_plot$sensitivities[2],
       col = "red", pch=16)

log_x_plot = plot.roc(train_val[,traininglabel],
                       as.numeric(predict(x_log$models[[2]],
                                          newdata = train_val, 
                                          probability = TRUE)),  
         print.auc = TRUE,  
         main = "Logistic Regression with X-axis Data")
points(log_x_plot$specificities[2], log_x_plot$sensitivities[2],
       col = "red", pch=16)

log_tree_plot = plot.roc(train_val[,traininglabel],
                       as.numeric(predict(x_tree$models[[2]], 
                                          newdata = train_val, 
                                          probability = TRUE)),
         print.auc = TRUE,  
         main = "Decision Tree with X-axis Data")
points(log_tree_plot$specificities[2], log_tree_plot$sensitivities[2],
       col = "red", pch=16)
@

<<>>==ROC curves of lda and qda
par(mfrow=c(2,2))
lda.block.roc = plot.roc(train_val[,traininglabel],
                       as.numeric(predict(lda.all.block$models[[8]], newdata = train_val, probability = TRUE)), print.auc = TRUE, main = "LDA with blocked data")
points(lda.block.roc$specificities[2], lda.block.roc$sensitivities[2],
       col = "red", pch=16)
qda.block.roc = plot.roc(train_val[,traininglabel],
                       as.numeric(predict(qda.all.block$models[[9]], newdata = train_val, probability = TRUE)), print.auc = TRUE, main = "QDA with blocked data")
points(qda.block.roc$specificities[2], qda.block.roc$sensitivities[2],
       col = "red", pch=16)

lda.x.roc = plot.roc(train_val2[,traininglabel],
                       as.numeric(predict(lda.all.xaxis$models[[2]], newdata = train_val2, probability = TRUE)), print.auc = TRUE, main = "LDA with x-axis data")
points(lda.x.roc$specificities[2], lda.x.roc$sensitivities[2],
       col = "red", pch=16)
qda.x.roc = plot.roc(train_val2[,traininglabel],
                       as.numeric(predict(qda.all.xaxis$models[[10]], newdata = train_val2, probability = TRUE)), print.auc = TRUE, main = "QDA with x-axis data")
points(qda.x.roc$specificities[2], qda.x.roc$sensitivities[2],
       col = "red", pch=16)
@

<<>>==specificities and sensitivities of lda and qda
lda.block.roc$specificities[2]
lda.block.roc$sensitivities[2]

qda.block.roc$specificities[2]
qda.block.roc$sensitivities[2]
@


\item (Bonus) Assess the fit using other relevant metrics.
<<>>= PPV and NPV function for assemenet
PPV <- function(prediction, obs){
  NPC = 0
  
  for (i in 1:length(prediction)){
    if (prediction[i] == 1) {
      NPC = NPC + 1
    }
  }
  
  TP = prediction[prediction==obs]
  NTP = length(TP[TP==1]) 
  return(NTP/NPC)
}

NPV <- function(prediction, obs){
  NNC = 0
  
  for (i in 1:length(prediction)){
    if (prediction[i] == -1) {
      NNC = NNC + 1
    }
  }

  NP = prediction[prediction==obs]
  NN = length(NP[NP==-1])
  return(NN/NNC)

@

<<>>== PPV and NPV values
PPV(predict(lda.all.block$models[[8]], newdata = test, probability = TRUE), 
    test$expertlabel)
NPV(predict(lda.all.block$models[[8]], newdata = test, probability = TRUE), 
    test$expertlabel)

PPV(predict(qda.all.block$models[[9]], newdata = test, probability = TRUE), 
    test$expertlabel)
NPV(predict(qda.all.block$models[[9]], newdata = test, probability = TRUE), 
    test$expertlabel)

PPV(predict(lda.all.xaxis$models[[2]], newdata = test, probability = TRUE), 
    test$expertlabel)
NPV(predict(lda.all.xaxis$models[[2]], newdata = test, probability = TRUE), 
    test$expertlabel)

PPV(predict(qda.all.xaxis$models[[10]], newdata = test, probability = TRUE), 
    test$expertlabel)
NPV(predict(qda.all.xaxis$models[[10]], newdata = test, probability = TRUE), 
    test$expertlabel)

@

\end{enumerate}

\section{Diagnostics (50 pts)}
\emph{Disclaimer:} The questions in this section are open-ended.
Be visual and quantitative! The gold standard arguments would be able to
convince National Aeronautics and  Space Administration (NASA) to use your
classification method---in which case Bonus points will be awarded.
\begin{enumerate}[label=(\alph*)]
\item Do an in-depth analysis of a good classification model
of your choice by showing some diagnostic plots or information related to
convergence or parameter estimation.

<<>>=Show the decision tree
rpart.plot(best_block_model$fold5.model$finalModel, box.palette = "Reds", tweak = 1.2)
@


\item For your best classification model(s), do you notice any patterns in the 
misclassification errors? Again, use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?

<<>>=Check for specific inaccurate predictions
misclassify_img1 = img1_df[img1_df$expertlabel != 0,]
misclassify_img1 = misclassify_img1[misclassify_img1$expertlabel != 
                             misclassify_img1$block_tree_predict,]

misclassify_img2 = img2_df[img2_df$expertlabel != 0,]
misclassify_img2 = misclassify_img2[misclassify_img2$expertlabel != 
                             misclassify_img2$block_tree_predict,]

misclassify_img3 = img3_df[img3_df$expertlabel != 0,]
misclassify_img3 = misclassify_img3[misclassify_img3$expertlabel != 
                             misclassify_img3$block_tree_predict,]

misclassify = rbind(misclassify_img1, misclassify_img2,
                    misclassify_img3)

misclassify_plot1 = ggplot(misclassify_img1, aes(x = x, y = y, color = factor(block_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 1 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predicted") + 
  theme_grey(base_size = 14)

misclassify_plot2 = ggplot(misclassify_img2, aes(x = x, y = y, color = factor(block_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 2 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predicted") + 
  theme_grey(base_size = 14)

misclassify_plot3 = ggplot(misclassify_img3, aes(x = x, y = y, color = factor(block_tree_predict))) +
  geom_point() + 
  scale_colour_manual(values = c("#00AFBB", "#FC4E07"),
                      labels = c("Not cloudy", "Cloudy")) + 
  labs(title = "Image 3 Cloud Regions", y = "y-coordinate", x = "x-coordinate",
       color = "Predicted") + 
  theme_grey(base_size = 14)
@

<<>>=Plot mispredictions for all images
grid.arrange(misclassify_plot1, misclassify_plot2, misclassify_plot3,
            nrow = 2, ncol = 2)
@

<<>>=Plot misprediction spread for the different features
xcoord = ggplot(misclassify, aes(x = expertlabel, y = x, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "x-coordinate")

ycoord = ggplot(misclassify, aes(x = expertlabel, y = y, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "y-coordinate")

NDAI = ggplot(misclassify, aes(x = expertlabel, y = NDAI, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "NDAI")

SD = ggplot(misclassify, aes(x = expertlabel, y = SD, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "SD")

CORR = ggplot(misclassify, aes(x = expertlabel, y = CORR, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "CORR")

DF = ggplot(misclassify, aes(x = expertlabel, y = DF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle DF")

CF = ggplot(misclassify, aes(x = expertlabel, y = CF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle CF")

BF = ggplot(misclassify, aes(x = expertlabel, y = BF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle BF")

AF = ggplot(misclassify, aes(x = expertlabel, y = AF, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle AF")

AN = ggplot(misclassify, aes(x = expertlabel, y = AN, fill = factor(expertlabel))) + 
  geom_boxplot() + 
  scale_fill_discrete(name = "Expert Label", labels = c("Not cloudy", "Cloudy")) + 
  labs(x = "Expert Label", y = "Radiance Angle AN")

grid.arrange(xcoord, ycoord, NDAI, CORR, SD,
             DF, CF, BF, AF, AN, ncol = 3, nrow = 4)
@

<<>>=Quantitative check
summary(misclassify)
@

<<>>=plots from different tuning parameters
features <- c("y", "x", "NDAI","SD","CORR")
rpart.selected.block = genericCV('rpart', features, traininglabel, data= train_val, grid = expand.grid(cp = seq(0, 0.2, 0.005)))
rpart2.selected.block = genericCV('rpart2', features, traininglabel, data= train_val, grid = expand.grid(.maxdepth = seq(1,10,1)))

features <- c("y", "x", "NDAI","SD","CORR")
rpart.selected.block.nongrid = genericCV('rpart', features, traininglabel, data= train_val)
rpart2.selected.block.nongrid = genericCV('rpart2', features, traininglabel, data= train_val)

features <- c("y", "x", "NDAI","SD","CORR", "DF", "CF", "BF", "AF", "AN")
rpart.all.block = genericCV('rpart', features, traininglabel, data= train_val, grid = expand.grid(cp = seq(0, 0.2, 0.005)))
rpart2.all.block = genericCV('rpart2', features, traininglabel, data= train_val, grid = expand.grid(.maxdepth = seq(1,10,1)))

rpart.all.block.nongrid = genericCV('rpart', features, traininglabel, data = train_val)

plot(rpart.selected.block$models[[3]], main = "Accuracy vs CP")
plot(rpart2.selected.block$models[[6]], main = "Accuracy vs tree depth")
```
@

<<>>=see difference in accuracies between tuning parameters and using default setting, and in different features

for (i in 1:10){
  print(max(rpart.selected.block$models[[i]]$results$Accuracy))
}

plot(rpart.selected.block$models[[1]])
plot(rpart.selected.block.nongrid$models[[1]])

for (i in 1:10){
  print(rpart.selected.block.nongrid$models[[i]]$results$Accuracy)
}

for (i in 1:10) {
print(max(rpart.selected.block.nongrid$models[[i]]$results))
}

for (i in 1:10) {
print(max(rpart.selected.block$models[[i]]$results))
}

for (i in 1:10) {
print(max(rpart2.selected.block.nongrid$models[[i]]$results$Accuracy))
}

for (i in 1:10) {
print(max(rpart2.selected.block$models[[i]]$results$Accuracy))
}
@
\item Based on parts 4(a) and 4(b), can you think of a better classifier?
How well do you think your model will work on future data without expert 
labels?
\item Do your results in parts 4(a) and 4(b) change as you modify the
way of splitting the data?
\item Write a paragraph for your conclusion.
\end{enumerate}

\section{Reproducibility (10 pts)}
In addition to a writeup of the above results, please provide a one-line link 
to a public GitHub repository containing everything necessary to reproduce
your writeup. Specifically, imagine that at some point an error is discovered
in the three image files, and a future researcher wants to check whether
your results hold up with the new, corrected image files. This researcher
should be able to easily re-run all your code and produce all your figures
and tables. This repository should contain:
\begin{enumerate}[label=(\roman*)]
  \item The pdf of the report,
  \item the raw Latex, Rnw or Word used to generate your report,
  \item your R code (with CVgeneric function in a separate R file),
  \item a README file describing, in detail, how to reproduce your paper
  from scratch (assume researcher has access to the images).
\end{enumerate}
You might want to take a look at the GitHub's tutorials 
\href{https://guides.github.com/}{https://guides.github.com/}.

\section*{Final remarks} % (fold)
\label{sec:final_remarks}
% As a reminder:
\begin{itemize}
  \item Make sure to read the instructions for the submission on Page
  1.
  \item Note that we will enforce a \textbf{zero tolerance policy for last
  minute / late requests (no emails please) this time.} Start early and
  plan ahead. If something
  is falling apart or not working, see us in office hours.
\end{itemize}

\end{document}